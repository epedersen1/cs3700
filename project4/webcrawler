#!/usr/bin/python3.6 -u

import sys
import socket
import json
import threading
import queue
import zlib
import time
from urllib.parse import urlparse
from html.parser import HTMLParser

DOMAIN = "fring.ccs.neu.edu"
PORT = 80
USERNAME = sys.argv[1] # "001719029"
PASSWORD = sys.argv[2] # "POYCPNDN"
THREADS = 50

server_address = (DOMAIN, PORT)

socks = {}
secret_flags_count = 0


class MyHTMLParser(HTMLParser):
    def handle_starttag(self, tag, attrs):
        attr = dict(attrs)
        if 'name' in attr and attr['name'] == 'csrfmiddlewaretoken':
            self.csrf_token = attr['value']
        if tag == 'a' and 'href' in attr:
            if not hasattr(self, 'links'):
                self.links = []
            self.links.append(attr['href'])
    def handle_data(self, data):
        global secret_flags_count
        if len(data) > 10 and data[:5] == 'FLAG:':
            secret_flags_count += 1
            print(data[6:])


def login(name, cookies):
    sock = socks[name]
    html = get(name, '/fakebook', cookies)

    parser = MyHTMLParser()
    parser.feed(html)
    csrf_token = parser.csrf_token

    body = "username=" + USERNAME + "&password=" + PASSWORD + "&csrfmiddlewaretoken=" + csrf_token
    message = "POST /accounts/login/?next=/fakebook/ HTTP/1.1\r\n"
    message += "Host: " + DOMAIN + "\r\n"
    message += "Content-Type: application/x-www-form-urlencoded\r\n"
    message += "Cookie: csrftoken=" + csrf_token + "\r\n"
    message += "Content-Length: " + str(len(body)) + "\r\n\r\n"
    message += body

    sock.send(message.encode("utf-8"))

    header = {}
    body = ''

    response = sock.recv(4096).decode("utf-8").split("\r\n")
    status_code = response[0].split()[1]

    if status_code == '500':
        login()

    for i in range(1, len(response)):
        if response[i] == '':
            body = response[i+1]
            break
        if response[i].split(': ')[0] == 'Set-Cookie':
            cookies.append(response[i].split(': ')[1].split(';')[0])
            continue
        header[response[i].split(': ')[0]] = response[i].split(': ')[1]


def get(name, url, cookies):
    global socks
    sock = socks[name]
    message = "GET " + url + " HTTP/1.1\r\nHost: " + DOMAIN + "\r\n"
    message += "Connection: Keep-Alive\r\n"
    message += "Accept-Encoding: gzip\r\n"
    if len(cookies):
        message += "Cookie: "
        for c in cookies:
            message += c + "; "
        message = message[:-2] + "\r\n"
    message += "\r\n"

    sock.send(message.encode("utf-8"))

    try:
        res = sock.recv(4096)
    except:
        get(name, url, cookies)

    if res == b'':
        sock.close()
        socks[name] = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        socks[name].connect(server_address)
        return get(name, url, cookies)
    header, body = res.split(b'\r\n\r\n')
    header = header.decode("utf-8").split('\r\n')

    headers = {}
    for h in header[1:]:
        if h.split(': ')[0] == 'Set-Cookie':
            cookies.append(h.split(': ')[1].split(';')[0])
            continue
        headers[h.split(': ')[0]] = h.split(': ')[1]

    status_code = header[0].split()[1]
    if status_code == '500':
        return get(name, url, cookies)

    if status_code == '301' or status_code == '302':
        return get(name, urlparse(headers['Location']).path, cookies)

    if status_code != '200':
        print("WTF!! Status Code: " + status_code)
        return ''

    if 'Content-Encoding' in headers and headers['Content-Encoding'] == 'gzip':
        body = zlib.decompress(body, 16+zlib.MAX_WBITS)

    if 'Transfer-Encoding' in header and header['Transfer-Encoding'] == 'chunked':
        print("chunked")
        print(body)

    return body.decode('utf-8')


def crawl(name, queue, added):
    socks[name] = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock = socks[name]
    sock.connect(server_address)

    cookies = []

    login(name, cookies)

    while secret_flags_count < 5:
        url = ''
        q_lock.acquire()
        len_q = queue.qsize()
        if not queue.empty():
            url = queue.get()
            q_lock.release()
        else:
            q_lock.release()
            break
        html = get(name, url, cookies)

        parser = MyHTMLParser()
        parser.feed(html)
        links = parser.links

        for link in links:
            url = urlparse(link)
            added_lock.acquire()
            if url.scheme == '' and url.netloc == '' and not (url.path in added) or \
                url.scheme == 'http' and url.netloc == DOMAIN and not (url.path in added):
                len_added = len(added)
                added[url.path] = 1
                added_lock.release()
                q_lock.acquire()
                queue.put(url.path)
                q_lock.release()
            else:
                added_lock.release()


class myThread (threading.Thread):
    def __init__(self, threadID, name, q, added):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.q = q
        self.added = added
    def run(self):
        crawl(self.name, self.q, self.added)


added = {'/fakebook': 1}
queue = queue.Queue()
queue.put('/fakebook')

added_lock = threading.Lock()
q_lock = threading.Lock()

threads = []

for i in range(THREADS):
   thread = myThread(i, "Thread-"+str(i), queue, added)
   thread.start()
   threads.append(thread)
   time.sleep(1./(i+1))

for t in threads:
   t.join()
