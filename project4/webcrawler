#!/usr/bin/python3.6 -u

import sys
import socket
import json
import threading
import queue
import zlib
import time
from urllib.parse import urlparse
from bs4 import BeautifulSoup

DOMAIN = "fring.ccs.neu.edu"
PORT = 80
USERNAME = "001719029"
PASSWORD = "POYCPNDN"
THREADS = 200

server_address = (DOMAIN, PORT)

# Set up the socket
socks = {}

def csrf(tag):
    return tag.has_attr('name') and tag['name'] == 'csrfmiddlewaretoken'


def login(name, cookies):
    sock = socks[name]
    html = get(name, '/fakebook', cookies)

    soup = BeautifulSoup(html, 'html.parser')
    csrf_token = soup.find(csrf)['value']

    body = "username=" + USERNAME + "&password=" + PASSWORD + "&csrfmiddlewaretoken=" + csrf_token
    message = "POST /accounts/login/?next=/fakebook/ HTTP/1.1\r\n"
    message += "Host: " + DOMAIN + "\r\n"
    message += "Content-Type: application/x-www-form-urlencoded\r\n"
    message += "Cookie: csrftoken=" + csrf_token + "\r\n"
    message += "Content-Length: " + str(len(body)) + "\r\n\r\n"
    message += body

    sock.send(message.encode("utf-8"))

    header = {}
    body = ''

    response = sock.recv(4096).decode("utf-8").split("\r\n")
    status_code = response[0].split()[1]

    if status_code == '500':
        login()

    for i in range(1, len(response)):
        if response[i] == '':
            body = response[i+1]
            break
        if response[i].split(': ')[0] == 'Set-Cookie':
            cookies.append(response[i].split(': ')[1].split(';')[0])
            continue
        header[response[i].split(': ')[0]] = response[i].split(': ')[1]


def get(name, url, cookies):
    global socks
    sock = socks[name]
    message = "GET " + url + " HTTP/1.1\r\nHost: " + DOMAIN + "\r\n"
    message += "Connection: Keep-Alive\r\n"
    message += "Accept-Encoding: gzip\r\n"
    if len(cookies):
        message += "Cookie: "
        for c in cookies:
            message += c + "; "
        message = message[:-2] + "\r\n"
    message += "\r\n"

    sock.send(message.encode("utf-8"))

    try:
        res = sock.recv(4096)
    except:
        get(name, url, cookies)

    if res == b'':
        sock.close()
        socks[name] = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        socks[name].connect(server_address)
        return get(name, url, cookies)
    header, body = res.split(b'\r\n\r\n')
    header = header.decode("utf-8").split('\r\n')

    headers = {}
    for h in header[1:]:
        if h.split(': ')[0] == 'Set-Cookie':
            cookies.append(h.split(': ')[1].split(';')[0])
            continue
        headers[h.split(': ')[0]] = h.split(': ')[1]

    status_code = header[0].split()[1]
    if status_code == '500':
        return get(name, url, cookies)

    if status_code == '301' or status_code == '302':
        return get(name, urlparse(headers['Location']).path, cookies)

    if status_code != '200':
        print("WTF!! Status Code: " + status_code)
        return ''

    if 'Content-Encoding' in headers and headers['Content-Encoding'] == 'gzip':
        body = zlib.decompress(body, 16+zlib.MAX_WBITS)

    if 'Transfer-Encoding' in header and header['Transfer-Encoding'] == 'chunked':
        print("chunked")
        print(body)

    return body.decode('utf-8')


def crawl(name, queue, added):

    socks[name] = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock = socks[name]
    sock.connect(server_address)

    cookies = []

    login(name, cookies)

    while True:
        url = ''
        q_lock.acquire()
        len_q = queue.qsize()
        if not queue.empty():
            url = queue.get()
            q_lock.release()
        else:
            q_lock.release()
            break
        html = get(name, url, cookies)

        soup = BeautifulSoup(html, 'html.parser')
        flag = soup.find_all(attrs={"class": "secret_flag"})
        if flag:
            print(flag[0].text[6:])

        links = soup.findAll('a')
        for link in links:
            href = link['href']
            url = urlparse(href)
            added_lock.acquire()
            if url.scheme == '' and url.netloc == '' and not (url.path in added) or \
                url.scheme == 'http' and url.netloc == DOMAIN and not (url.path in added):
                len_added = len(added)
                added[url.path] = 1
                added_lock.release()
                # if len_added % 100 == 0:
                    # print(f'{name} url: {url.path:30}   added: {len_added:5d}')
                q_lock.acquire()
                queue.put(url.path)
                q_lock.release()
            else:
                added_lock.release()


class myThread (threading.Thread):
    def __init__(self, threadID, name, q, added):
        threading.Thread.__init__(self)
        self.threadID = threadID
        self.name = name
        self.q = q
        self.added = added
    def run(self):
        # print("Starting " + self.name)
        crawl(self.name, self.q, self.added)
        # print("Exiting " + self.name)


added = {'/fakebook': 1}
queue = queue.Queue()
queue.put('/fakebook')

added_lock = threading.Lock()
q_lock = threading.Lock()

threads = []

# Create new threads
for i in range(THREADS):
   thread = myThread(i, "Thread-"+str(i), queue, added)
   thread.start()
   threads.append(thread)
   time.sleep(1./(i+1))

# Wait for queue to empty
# while not queue.empty():
#    pass

# Notify threads it's time to exit
exitFlag = 1

# Wait for all threads to complete
for t in threads:
   t.join()
